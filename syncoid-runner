#!/usr/bin/env python3
"""
Syncoid Automation Script
Parses configuration and executes ZFS replications on schedule
"""

import configparser
import argparse
import subprocess
from datetime import datetime, timedelta
import sys
import os
from termcolor import colored
import re
import asyncio
import zlib

# Global flag to control output verbosity
QUIET_MODE = True

FREQUENCY_MULTIPLIERS = {
    'm': 60,
    'h': 3600,
    'D': 86400,
    'W': 604800,
    'M': 2592000,
    'Y': 31536000
}

def print_message(message, is_error=False, force_print=False):
    """Print message based on verbosity settings
    
    Args:
        message: The message to print
        is_error: If True, always print regardless of QUIET_MODE
        force_print: If True, always print regardless of QUIET_MODE
    """
    global QUIET_MODE
    if is_error or force_print or not QUIET_MODE:
        print(message)

CRON_MAPPING = {
    'H': 'hourly',
    'D': 'daily',
    'W': 'weekly',
    'M': 'monthly',
    'Y': 'yearly'
}

def get_staggered_schedule(dataset_name, freq_unit):
    """Get staggered hour and day based on dataset name hash"""
    # Use hash of dataset name for consistent but distributed scheduling
    name_hash = hash(dataset_name)
    
    # For hourly backups, we don't need to calculate anything
    if freq_unit in ['h', 'm']:
        return 0, 0
        
    # Calculate hour: 1-23 for daily/weekly/monthly/yearly
    # Use prime numbers for better distribution
    hour = (abs(name_hash) % 23) + 1
    
    # For weekly backups, calculate day: 1-7 (Mon-Sun)
    # Avoid weekend days for most backups
    if freq_unit == 'W':
        # Use a different hash calculation for days to avoid correlation with hours
        day_hash = hash(dataset_name + '_day')
        # Distribute 70% of backups on weekdays (1-5) and 30% on weekends (0,6)
        if abs(day_hash) % 10 < 7:  # 70% chance
            day = (abs(day_hash) % 5) + 1  # Monday-Friday
        else:  # 30% chance
            day = [0, 6][abs(day_hash) % 2]  # Saturday/Sunday
    else:
        day = 0
        
    return hour, day

def parse_frequency(freq_str):
    """Parse frequency string into value and unit.
    Valid formats:
    - Nm or Nh for minutes/hours (N is a number)
    - ND for days
    - NW for weeks
    - NM for months
    - NY for years
    """
    if not freq_str:
        return None, None
        
    match = re.match(r'(\d+)([hmDWMY])', freq_str)
    if not match:
        print_message(f"Invalid frequency format: {freq_str}. Must be number followed by one of: h m D W M Y", is_error=True)
        return None, None
        
    value = int(match.group(1))
    unit = match.group(2)
    return value, unit

def parse_retention(retention_str):
    """Parse retention policy string into a list of (duration, count) tuples
    
    Format: duration:count,duration:count,...
    Example: 24h:8,7d:7,30d:4
    """
    if not retention_str:
        return []
    
    policies = []
    for policy in retention_str.split(','):
        duration_str, count_str = policy.split(':')
        count = int(count_str)
        value, unit = parse_frequency(duration_str)
        if value and unit:
            policies.append((value, unit, count))
    return policies

def cleanup_snapshots(dataset, retention_policies, is_remote=False):
    """Clean up snapshots based on retention policy"""
    if ':' in dataset:
        host, dataset_path = dataset.split(':', 1)
        cmd = ['ssh', host, f'zfs list -t snapshot -o name,creation -s creation -r -H "{dataset_path}"']
    else:
        cmd = ['zfs', 'list', '-t', 'snapshot', '-o', 'name,creation', '-s', 'creation', '-r', dataset, '-H']
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print_message(f"Error listing snapshots for {dataset}: {result.stderr}", is_error=True)
        return

    snapshots = []
    for line in result.stdout.splitlines():
        if 'syncoid' in line:
            name, creation = line.split('\t')
            try:
                # Try multiple date formats
                for fmt in ['%c', '%a %b %d %H:%M %Y', '%Y-%m-%d %H:%M:%S']:
                    try:
                        creation_date = datetime.strptime(creation.strip(), fmt)
                        snapshots.append((name, creation_date))
                        break
                    except ValueError:
                        continue
            except Exception as e:
                print_message(f"Error parsing date '{creation}': {e}", is_error=True)
                continue

    # Group snapshots by age
    now = datetime.now()
    to_delete = set()
    kept = set()

    for duration_value, duration_unit, keep_count in retention_policies:
        duration_seconds = duration_value * FREQUENCY_MULTIPLIERS.get(duration_unit, 0)
        if duration_seconds == 0:
            continue

        age_threshold = now - timedelta(seconds=duration_seconds)
        period_snapshots = [(name, date) for name, date in snapshots 
                          if date <= now and date > age_threshold 
                          and name not in kept and name not in to_delete]
        
        # Keep the newest keep_count snapshots in this period
        period_snapshots.sort(key=lambda x: x[1], reverse=True)
        kept.update(name for name, _ in period_snapshots[:keep_count])
        to_delete.update(name for name, _ in period_snapshots[keep_count:])

    # Delete snapshots that don't match retention policy
    for snapshot in to_delete:
        if ':' in dataset:
            host, _ = dataset.split(':', 1)
            cmd = ['ssh', host, f'zfs destroy "{snapshot}"']
        else:
            cmd = ['zfs', 'destroy', snapshot]
            
        print_message(f"Deleting snapshot: {snapshot}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print_message(f"Error deleting snapshot {snapshot}: {result.stderr}", is_error=True)

async def verify_snapshot_age(dataset, max_age_seconds, is_remote=False):
    """Verify the age of the latest snapshot on a dataset.

    Args:
        dataset: The ZFS dataset to check (can be remote:pool/dataset)
        max_age_seconds: Maximum allowed age of the snapshot in seconds
        is_remote: Boolean, True if the dataset is on a remote host

    Returns:
        True if the latest snapshot is within the max_age, False otherwise.
    """
    try:
        if is_remote:
            host, dataset_path = dataset.split(':', 1)
            cmd = f'ssh {host} "zfs list -t snapshot -r -H -o name,creation -s creation {dataset_path} | tail -n 1"'
        else:
            cmd = f'zfs list -t snapshot -r -H -o name,creation -s creation {dataset} | tail -n 1'

        process = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            print_message(f"Error listing snapshots for {dataset}: {stderr.decode()}", is_error=True)
            return False

        output = stdout.decode().strip()
        if not output:
            print_message(f"No snapshots found for {dataset}", is_error=True)
            return False

        name, creation_str = output.split('\t')
        try:
            # Attempt to parse the creation date using different formats
            creation_date = None
            for fmt in ['%c', '%a %b %d %H:%M %Y', '%Y-%m-%d %H:%M:%S']:
                try:
                    creation_date = datetime.strptime(creation_str.strip(), fmt)
                    break
                except ValueError:
                    continue

            if creation_date is None:
                raise ValueError("No valid date format found")

            age_seconds = (datetime.now() - creation_date).total_seconds()

            if age_seconds <= max_age_seconds:
                print_message(colored(f"Latest snapshot for {dataset} is within the acceptable age.", "green"))
                return True
            else:
                print_message(colored(f"Latest snapshot for {dataset} is too old.", "red"), is_error=True)
                return False

        except Exception as e:
            print_message(f"Error processing snapshot data for {dataset}: {e}", is_error=True)
            return False

    except Exception as e:
        print_message(f"General error verifying snapshot age for {dataset}: {e}", is_error=True)
        return False

class SyncoidConfig:
    def __init__(self, config_path):
        self.config = configparser.ConfigParser()
        print_message(colored(f"Reading config from: {config_path}", 'blue'))
        self.config.read(config_path)
        self.templates = {}
        self.datasets = []
        
        # Parse templates first
        print_message(colored("\nFound sections:", 'blue'))
        for section in self.config.sections():
            print_message(f"  {section}")
        
        print_message(colored("\nParsing templates:", 'blue'))
        for section in self.config.sections():
            if section.startswith('template_'):
                print_message(f"  Found template: {section}")
                self.templates[section] = dict(self.config[section])
                print_message(f"    Settings: {self.templates[section]}")
                
        # Parse datasets
        print_message(colored("\nParsing datasets:", 'blue'))
        for section in self.config.sections():
            if not section.startswith('template_') and section != 'DEFAULT':
                print_message(f"  Processing dataset: {section}")
                base_dataset = {
                    'source': section.strip('[]'),
                    'options': [],
                    'template': None
                }
                
                destinations = []
                # First pass: collect all non-destination settings
                for key, value in self.config[section].items():
                    print_message(f"    Found key: {key} = {value}")
                    if key.startswith('destination_'):
                        destinations.append(value)
                    elif key == 'use_template':
                        base_dataset['template'] = f"template_{value.strip()}"
                        print_message(f"    Using template: template_{value.strip()}")
                    elif key.startswith('option_'):
                        opt_name = '--' + key[7:].replace('_', '-')
                        # Handle boolean options vs options with values
                        if value.lower() == 'true':
                            base_dataset['options'].append(opt_name)
                        else:
                            base_dataset['options'].append(f"{opt_name}={value}")
                
                # Create dataset entries for each destination
                for dest in destinations:
                    dataset_copy = base_dataset.copy()
                    dataset_copy['options'] = base_dataset['options'].copy()
                    dataset_copy['destination'] = dest
                    self.datasets.append(dataset_copy)
                    print_message(f"    Added destination: {dest}")
                
                print_message(f"    Final dataset configs:")
                for ds in [d for d in self.datasets if d['source'] == base_dataset['source']]:
                    print_message(f"      {ds}")

    def generate_commands(self):
        """Generate syncoid commands for all datasets"""
        commands = []
        for ds in self.datasets:
            if 'destination' not in ds:
                continue
                
            # Skip datasets using the ignore template
            if ds.get('template') == 'template_ignore':
                print_message(colored(f"Skipping {ds['source']}: using ignore template", 'yellow'))
                continue
                
            cmd = ['syncoid', '--no-privilege-elevation']
            cmd += ds['options']
            cmd += [ds['source'], ds['destination']]
            
            commands.append(' '.join(cmd))
            
        return commands

    def generate_cron(self):
        """
        Generate cron entries for automated backups based on dataset templates.
        This method creates a list of cron job entries for datasets, using their
        associated templates to determine the backup frequency and schedule. The
        generated cron jobs are formatted for use with the `cron` daemon.
        Returns:
            str: A string containing the generated cron job entries, including
                 a header and environment variable definitions.
        Notes:
            - Datasets with the 'template_ignore' template are skipped.
            - Templates must define a valid 'frequency' to be processed.
            - Supported frequency units:
                - 'm': Minutes
                - 'h': Hours
                - 'D': Days
                - 'W': Weeks
                - 'M': Months
                - 'Y': Years
            - The method staggers schedules to distribute backup jobs over time.
        Example:
            The generated cron job for a dataset with a daily frequency might look like:
            ```
            0 3 */1 * * root /usr/local/bin/syncoid-runner --dataset my_dataset
            ```
            This schedules a daily backup at 3:00 AM.
        Raises:
            None: This method does not raise exceptions but skips invalid datasets
                  or templates with warnings printed to the console.
        """
        """Generate cron entries based on template frequencies"""
        cron_jobs = []
        # Add cron file header
        cron_jobs.append("# Syncoid automated backup cron jobs")
        cron_jobs.append("# Generated on " + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        cron_jobs.append("SHELL=/bin/bash")
        cron_jobs.append("PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin")
        cron_jobs.append("")
        
        # Group datasets by source and template to avoid duplicate cron entries
        source_templates = {}
        for ds in self.datasets:
            if 'template' not in ds or not ds['template']:
                continue
                
            # Skip datasets using the ignore template
            if ds['template'] == 'template_ignore':
                print_message(colored(f"Skipping {ds['source']}: using ignore template", 'yellow'))
                continue
                
            template = self.templates.get(ds['template'])
            if not template or 'frequency' not in template:
                continue

            source = ds['source']
            if source not in source_templates:
                source_templates[source] = template
                
                print_message(colored(f"Checking dataset: {source}", 'cyan'))
                print_message(f"Using template: {ds['template']}")
                if template:
                    print_message(colored(f"Found template settings: {template}", 'green'))
                else:
                    print_message(colored("Template not found!", 'red'), is_error=True)
                    continue
                
                freq_value, freq_unit = parse_frequency(template['frequency'])
                if freq_value is None:
                    continue
                    
                hour, day = get_staggered_schedule(source, freq_unit)
                
                if freq_unit == 'm':
                    # Minute-based frequency
                    cron_time = f'*/{freq_value} * * * *'
                    schedule_msg = f"every {freq_value} minute(s)"
                elif freq_unit == 'h':
                    # Hourly backups run on the hour
                    cron_time = f'0 */{freq_value} * * *'
                    schedule_msg = f"every {freq_value} hour(s)"
                elif freq_unit == 'D':
                    # Daily backups run at a staggered hour
                    cron_time = f'0 {hour} */{freq_value} * *'
                    schedule_msg = f"daily at {hour:02d}:00"
                elif freq_unit == 'W':
                    # Weekly backups run on a staggered day and hour
                    cron_time = f'0 {hour} * * {day}'
                    schedule_msg = f"weekly on {['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'][day]} at {hour:02d}:00"
                elif freq_unit == 'M':
                    # Monthly backups run on the 1st at a staggered hour
                    cron_time = f'0 {hour} 1 */{freq_value} *'
                    schedule_msg = f"monthly on the 1st at {hour:02d}:00"
                elif freq_unit == 'Y':
                    # Yearly backups run on Jan 1st at a staggered hour
                    cron_time = f'0 {hour} 1 1 *'
                    schedule_msg = f"yearly on Jan 1st at {hour:02d}:00"
                else:
                    print_message(colored(f"Unknown frequency unit: {freq_unit}", 'red'), is_error=True)
                    continue
                
                print_message(colored(f"Scheduling backup for {source} {schedule_msg}", 'blue'))
                cron_cmd = f'{cron_time} root /usr/local/bin/syncoid-runner --dataset {source}'
                cron_jobs.append(cron_cmd)
            
        return "\n".join(cron_jobs) + "\n"

async def run_command_async(cmd, log):
    process = await asyncio.create_subprocess_shell(
        cmd,
        stdout=log,
        stderr=log
    )
    await process.wait()

async def check_pool_exists(pool, is_remote=False):
    """Check if a ZFS pool exists
    
    Args:
        pool: Pool name or remote:pool string
        is_remote: Whether this is a remote pool
        
    Returns:
        bool: True if pool exists, False otherwise
    """
    if ':' in pool:
        # Remote pool
        host, pool_path = pool.split(':', 1)
        # Extract just the pool name (before first slash)
        pool_name = pool_path.split('/')[0]
        cmd = f'ssh {host} "zpool list -H {pool_name}"'
    else:
        # Local pool
        # Extract just the pool name (before first slash)
        pool_name = pool.split('/')[0]
        cmd = f'zpool list -H {pool_name}'
    
    try:
        process = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        _, stderr = await process.communicate()
        return process.returncode == 0
    except Exception as e:
        print_message(f"Error checking pool {pool_name}: {e}", is_error=True)
        return False

async def main():
    parser = argparse.ArgumentParser(description='Run syncoid backup jobs')
    parser.add_argument('--config', default='/etc/syncoid/syncoid.conf', help='Path to config file')
    parser.add_argument('--dataset', help='Only process specific dataset')
    parser.add_argument('--generate-cron', action='store_true', help='Generate cron jobs')
    parser.add_argument('--dry-run', action='store_true', help='Show commands without executing')
    parser.add_argument('--cleanup', action='store_true', help='Run retention policy cleanup')
    parser.add_argument('--help-details', action='store_true', 
                   help='Display detailed documentation about this script')
    parser.add_argument('--verify', action='store_true', help='Verify the age of the latest snapshot on each destination')
    
    args = parser.parse_args()
    
    if args.help_details:
        print_detailed_help()
        return
    
    global QUIET_MODE
    QUIET_MODE = not args.dry_run
    
    try:
        config = SyncoidConfig(args.config)
    except Exception as e:
        print_message(f"Error reading config: {e}", is_error=True)
        sys.exit(1)

    if args.cleanup:
        for ds in config.datasets:
            template = config.templates.get(ds['template'])
            if template:
                local_retention = template.get('retention_local', '')
                remote_retention = template.get('retention_remote', '')
                
                # Clean up local snapshots
                if local_retention:
                    local_policies = parse_retention(local_retention)
                    cleanup_snapshots(ds['source'], local_policies)
                
                # Clean up remote snapshots
                if remote_retention and ':' in ds['destination']:
                    remote_policies = parse_retention(remote_retention)
                    cleanup_snapshots(ds['destination'], remote_policies, is_remote=True)
        return

    if args.generate_cron:
        # Force verbose output for cron generation
        QUIET_MODE = False
        cron_content = config.generate_cron()
        if args.dry_run:
            print_message(colored("\nCron entries that would be generated:", "cyan"))
            print_message(colored("-" * 50, "cyan"))
            print_message(cron_content)
        else:
            cron_file = '/etc/cron.d/syncoid-jobs'
            try:
                with open(cron_file, 'w') as f:
                    f.write(cron_content)
                print_message(colored(f"\nCron entries written to {cron_file}", "green"))
                print_message(colored("Restarting cron service...", "blue"))
                subprocess.run(['systemctl', 'restart', 'cron'], check=True)
                print_message(colored("Cron service restarted successfully!", "green"))
            except Exception as e:
                print_message(colored(f"Error writing cron file: {e}", "red"), is_error=True)
                sys.exit(1)
        return

    commands = config.generate_commands()
    
    if args.dataset:
        commands = [cmd for cmd in commands if f' {args.dataset} ' in cmd]
    
    if not commands:
        print_message(colored("No matching commands found.", "yellow"))
        return

    if args.dry_run:
        print_message(colored("\nCommands that would be executed:", "cyan"))
        print_message(colored("-" * 50, "cyan"))
        for i, cmd in enumerate(commands, 1):
            print_message(colored(f"\nCommand {i}:", "green"))
            print_message(f"Dataset: {colored(cmd.split()[-2], 'yellow')}")
            print_message(f"Destination: {colored(cmd.split()[-1], 'yellow')}")
            print_message("Options:")
            options = [opt for opt in cmd.split() if opt.startswith('--')]
            for opt in options:
                print_message(f"  {colored(opt, 'blue')}")
            print_message(colored("\nFull command:", "white"))
            print_message(f"{cmd}\n")
        print_message(colored("-" * 50, "cyan"))
        return
    
    # Verify snapshot age if --verify is enabled
    if args.verify:
        successes = 0
        failures = 0
        for ds in config.datasets:
            template = config.templates.get(ds['template'])
            if not template:
                print_message(colored(f"No template found for {ds['source']}, skipping verification.", "yellow"))
                failures += 1
                continue

            # Determine max_age based on frequency
            frequency = template.get('frequency')
            if not frequency:
                print_message(colored(f"No frequency defined in template {ds['template']}, skipping verification.", "yellow"))
                failures += 1
                continue

            freq_value, freq_unit = parse_frequency(frequency)
            if freq_value is None or freq_unit is None:
                print_message(colored(f"Invalid frequency format in template {ds['template']}, skipping verification.", "yellow"))
                failures += 1
                continue

            max_age_seconds = freq_value * FREQUENCY_MULTIPLIERS.get(freq_unit, 0)
            if max_age_seconds == 0:
                print_message(colored(f"Invalid frequency unit {freq_unit} in template {ds['template']}, skipping verification.", "yellow"))
                failures += 1
                continue

            # Determine if the destination is remote
            is_remote = ':' in ds['destination']

            # Verify the snapshot age
            if await verify_snapshot_age(ds['destination'], max_age_seconds, is_remote):
                successes += 1
            else:
                failures += 1

        print_message(colored(f"\nSnapshot verification complete: {successes} successful, {failures} failed.", "cyan"))
        return # Exit after verification

    for cmd in commands:
        dataset = cmd.split()[-2]
        destination = cmd.split()[-1]
        
        # Check if destination pool exists
        if not await check_pool_exists(destination):
            print_message(colored(f"Destination pool for {destination} does not exist! Skipping.", "red"), is_error=True)
            continue
            
        log_file = f'/var/log/syncoid/{dataset}.log'
        dataset_dir = os.path.dirname(log_file)
        if not os.path.exists(dataset_dir):
            os.makedirs(dataset_dir)
        with open(log_file, 'a') as log:
            print_message(f"Running: {cmd}")
            try:
                await run_command_async(cmd, log)
            except Exception as e:
                print_message(f"Error executing command: {e}", is_error=True)
                sys.exit(1)

def print_detailed_help():
    """Display detailed help information about the script"""
    help_text = """
Syncoid Runner - ZFS Replication Automation Tool
================================================

DESCRIPTION
-----------
This tool automates ZFS replication using the syncoid utility. It reads a configuration
file to determine source datasets, destinations, and replication options.

USAGE
-----
syncoid-runner [OPTIONS]

OPTIONS
-------
--config PATH             Path to the configuration file (default: /etc/syncoid/syncoid.conf)
--dataset DATASET         Only process the specified dataset
--generate-cron           Generate cron jobs based on configuration
--dry-run                 Show commands without executing them
--cleanup                 Run retention policy cleanup on snapshots
--help-details            Display this detailed help information
--verify                  Verify the age of the latest snapshot on each destination

CONFIGURATION FILE FORMAT
------------------------
The configuration file uses INI format with sections for each dataset and templates.

Template sections start with 'template_' and define common settings:
[template_daily]
frequency = 1D
retention_local = 24h:7,7D:4
retention_remote = 7D:4,30D:3

Dataset sections define the source dataset (as the section name) and destinations:
[tank/data]
use_template = daily
destination_1 = backup:tank/data
option_create-bookmark = true

EXAMPLES
--------
# Run all replications defined in config
syncoid-runner

# Only replicate a specific dataset
syncoid-runner --dataset tank/data

# Generate cron entries based on configured frequencies
syncoid-runner --generate-cron

# Show commands that would be executed without actually running them
syncoid-runner --dry-run

# Clean up snapshots according to retention policies
syncoid-runner --cleanup

# Verify the age of the latest snapshot on each destination
syncoid-runner --verify

For more information, visit: https://github.com/jimsalterjrs/sanoid/
"""
    print(help_text)

if __name__ == '__main__':
    asyncio.run(main())